<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>When and How Should RL Agents Explore | Green Learning</title>
<meta name="keywords" content="language-model, vision-language-model, vision-model" />
<meta name="description" content="Developing reinforcement learning agents that can explore new ways of solving complex tasks and diverse environments is an open problem for current research. I will introduce two recently proposed techniques for tackling the exploration problem and demonstrate their effectiveness in the classic board game Backgammon. Focusing on the ‘when’ and ‘how’ your RL agent should explore, we will try deonstrating, can significantly improve sample efficiency (i.e. reduce waiting/training time) and creates the best agent.">
<meta name="author" content="Dean Ninalga">
<link rel="canonical" href="https://ninalgad.github.io/posts/2022-07-29-mitc/" />
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.949307ebc5cfbfc5e27c5ec3d332ce0be9137ec4137e02aefc52cf0267172e22.css" integrity="sha256-lJMH68XPv8XifF7D0zLOC&#43;kTfsQTfgKu/FLPAmcXLiI=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js" integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5&#43;kdJvBz5iKbt6B5PJI="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://ninalgad.github.io/olive.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://ninalgad.github.io/olive.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://ninalgad.github.io/olive.png">
<link rel="apple-touch-icon" href="https://ninalgad.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://ninalgad.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-8161570-5', 'auto');
	
	ga('send', 'pageview');
}
</script><meta property="og:title" content="The When and How Your RL Should Explore" />
<meta property="og:description" content="Developing reinforcement learning agents that can explore new ways of solving complex tasks and diverse environments is an open problem for current research. I will introduce two recently proposed techniques for tackling the exploration problem and demonstrate their effectiveness in the classic board game Backgammon. Focusing on the ‘when’ and ‘how’ your RL agent should explore, we will try deonstrating, can significantly improve sample efficiency (i.e. reduce waiting/training time) and creates the best agent." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://ninalgad.github.io/posts/2022-07-29-mitc/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-07-29T15:10:30-07:00" />
<meta property="article:modified_time" content="2022-07-29T15:10:30-07:00" />


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://ninalgad.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "The When and How Your RL Should Explore",
      "item": "https://ninalgad.github.io/posts/2022-08-29-exrl/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "The When and How Your RL Should Explore",
  "name": "Improving Datacenters Energy Efficiency",
  "description": "Developing the the best RL agent is the same as finding the best way an RL agent should explore.",
  "keywords": [
    "reinforcement learning", "exploration", "dqn"
  ],
  "articleBody": "",
  "wordCount" : "1000",
  "inLanguage": "en",
  "datePublished": "2022-08-29T15:10:30-07:00",
  "dateModified": "2022-08-29T15:10:30-07:00",
  "author":{
    "@type": "Person",
    "name": "Dean Ninalga"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://ninalgad.github.io/posts/2022-07-29-mitc/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Green Learning",
    "logo": {
      "@type": "ImageObject",
      "url": "https://ninalgad.github.io/olive.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://ninalgad.github.io/" accesskey="h" title="Green Learning (Alt + H)">Green Learning</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://ninalgad.github.io/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <!-- TABS
            <li>
                <a href="https://ninalgad.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://ninalgad.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://ninalgad.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://ninalgad.github.io/faq" title="FAQ">
                    <span>FAQ</span>
                </a>
            </li>
            -->
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      The When and How Your RL Should Explore
    </h1>
    <div class="post-meta"><span title='2022-07-29 15:10:30 -0700 PDT'>August 29, 2022</span>&nbsp;·&nbsp;29 min read&nbsp;·&nbsp;Dean Ninalga

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#intro" aria-label="intro">Introduction to RL explorarion</a></li>
                <li>
                    <a href="#bay-exp" aria-label="bay-exp">The Bayesian Approach | Robust DQN</a></li>
                <li>
                    <a href="#band-exp" aria-label="band-exp">Bandit Exploration</a></li>
                <li>
                    <a href="#experiments" aria-label="experiments">Experiments</a></li>
                <li>
                    <a href="#conclusion" aria-label="conlusion">Conclusion</a>
                </li>
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content">

<h1 id="intro">Introduction<a hidden class="anchor" aria-hidden="true" href="#goal-def">#</a></h1>
      <p>
In finding the optimal way to accomplish a task, one needs to explore their options in some way. We can state the problem of exploration (POE) as follows:
      </p>
      <blockquote>
      How do we gather enough facts such that we can obtain the most probable conclusion? 
      </blockquote>
<p>
Accordingly, the only way to develop confidence in predicting the most likely conclusion is to exploit the information we already have. For example, if you want to find the most efficient way to walk to the closest bus station from your house, you likely would prefer the route with the main streets and pathways you have previously taken (especially in the absence of a map on your phone GPS). However, the most efficient way is not necessarily the most familiar. This dilemma is known as the Exploitation vs. Exploration problem. Difficult to solve and solutions might be mostly domain specific.
      </p>
<img src="bus-stop.png" alt="Bus Stop">
      <p>
My aim in this post is to argue in favor of a bayesian treatment of POE. Specifically, I argue that Bayesian Reinforcement learning is the best solution to probabilistic environments, where experience is conditional on random events. Namely, probabilistic environments such as the board game Backgammon are too unpredictable for some agents to adequately solve, as any board position depends on a sequence of preceding rolls of two six-sided dice. Indeed, solving simulated probabilistic environments is much more interesting than using their deterministic counterparts as real-life decision-making often relies on non-deterministic factors.
      </p>
      <p>
I begin my investigation by articulating the Bayesian paradigm for reinforcement learning and recently proposed variants. Then, I briefly describe the recent work published by the company Deepmind directly addressing POE. Finally, we compare the performance of the two methodologies and demonstrate the superiority of the proposed bayesian methods in the experiments section.
      </p>
<h1 id="bay-exp">The Bayesian Approach - Robust DQN<a hidden class="anchor" aria-hidden="true" href="#bay-exp">#</a></h1>
<p>Although not as popular as the out-of-the-box approaches such as DQN, bayesian RL techniques can perform extremely well despite their extra theoretical baggage. There are many ways one can possibly include prior information in the RL framework, however here we will only look at works that add prior knowledge over the parameters $\theta$ with distribution $p(\theta)$. In the paper "Deep Robust Kalman Filter" Shashua & Mannor (2017) modified the DQN framework by incorporating information from the uncertainty in state transitions. The authors introduce their methods by defining the maximum a-posteriori (MAP) estimator of $\theta$ given a dataset of observations $O$ is given by:
    </p>
<div>
$$\theta^{MAP}=\arg\max\log p(\theta| O)=\arg\max\log p(O| \theta) + \log p(\theta)$$
</div>
      <p>Recall the opbective function, the <em>squared nominal Bell TD error at a given time step $t$</em> </p>
 <div>
$$L(\theta| \delta_{\theta^{\prime}}, y_{\theta^{\prime}})=\frac{1}{2}\mathbb{E}_{o\sim p}\big[\big(\delta_{\theta^{\prime}}(\theta,o)\big)^2\big]$$
</div>
      <p>where above $o$ is the environment obervation, $p$ is a network policy, $\theta^{\prime}$ is old model parameters, and we exclude the time index $t$ for clarity. Under this new Baysian paradigm the functional parameters in the above loss are given by:</p>
 <div>
$$\delta_{\theta^{\prime}}^{\text{robust}}(\theta, o) = y_{\theta^{\prime}}^{\text{robust}}(o)-Q(s,a;\theta)$$

$$y_{\theta^{\prime}}^{\text{robust}}(o)=r+\gamma\min_{p\in P}\sum_{s^{\prime}\in\tilde{S}}p(s^{\prime}| s,a)\max_{a^{\prime}} Q(s^{\prime},a^{\prime};\theta^{\prime})$$
</div>
      <p>
where $r$ is the reward at time step $t$, $\gamma$ is the discount hyperparameter and $\tilde{S}$ is all the possible states the we could arrive at if we tried every single possible action at this current state $s$. At a high level, $\delta$ is the same  <em>advantage</em> as in the DQN  case however much more work has to be done to understand the robust network targets $y_{\theta^{\prime}}^{\text{robust}}$. The authors has this to say:  
          </p>
<blockquote cite="https://arxiv.org/pdf/1703.02310.pdf"><p>It looks for worst case transitions that may
reduce the value of the expected Q-function, and sets the
robust target label value according to the minimal expectation. In return, the agent that receives these robust target
labels, learns how to act optimally over these transitions.</p></blockquote>
      <p>when talking about the targets $y_{\theta^{\prime}}^{\text{robust}}$. Using this as the loss agents are subsequently more robust and less overconfident as they explicitly account for a worst-case when training. </p>
    <p>Baysian methods in recent leturature seem to be very effective in preventing overconfidence. Indeed, Hein et. al. (2019) highlight the overconfidence problem found in neural nets commonly used in RL.  More concretely Hein et. al. (2019) prove:
      </p>
 <div>
$$\lim_{\delta\to\infty}\text{softmax}(f(\delta\mathbf{x}))_{i}=1 $$
</div>
      <p>Where $\mathbf{x}$ is a datum, $f$ is a neural net and, $\delta > 0$ and $i\in \{1, .., k\}$ are real numbers. In plain-english the above equation suggests that if $\delta\mathbf{x}$ is <em>far</em> enough away (from the training distribution of $f$) then a typical neural network will become overconfident, predicting a probability of 1 for the $i^{th}$ class. Subsequently, a year later Kristiadi et. al. (2020) argue that under some settings, bayesian methods can address overfitting in their paper fittingly named: “Being Bayesian, Even Just a Bit, Fixes Overconfidence in ReLU Networks”.
</p>
<p>In the vein of <em>trying</em> to be a little bayesian Derman et. al. (2019) proposed a bayesian approach to DQN. Their work is mainly based on the framework of Shashua \& Mannor (2017). The worst-case is explicitly defined in the loss in addition to a newly derived distribution over the Q-values and the incorporation of dynamic programming. We will only use this bayesian method and compare it against all the other non-bayesian methods later on in this post.</p>

<h2 id="urbe">DQN - URBE<a hidden class="anchor" aria-hidden="true" href="#urbe">#</a></h2>
      <p>Here we will go into some details about the very unique components in the algorithim in Derman et. al. (2019). named "DQN - Uncertainty Robust Bellman Equation" (DQN - URBE). We will explain some of the motivations of parts and the algorithim in just enough detail to make  implementation easier. Then we will provide the peudocode at the end of this section.</p>
<p>Let $p_{sa}^{h}$ where the ith element is the probability that we arrive at a next new state $s^{\prime}_{i}$ taking action $a$ in state $s$, where $p_{sa}^{h}$ is not necessarily constant size. Then let us define a prior on these probabilities:</p>
      <dir>$$p_{sa}^{h}|\mathbb{F}_{t}\sim\text{Dir}(\varphi_{sa}+n_{sa}^{h})$$</dir>
      <p>for $\varphi_{sa}$ is a vecor of priors over the transition probabilities $p_{sa}$ and $\mathbb{F}_{t}$ is descried as:</p>
<blockquote cite="https://arxiv.org/pdf/1905.08188.pdf"><p><em>as a minimal sigma-algebra that
contains all of the available information up to episode t
(e.g. all observed states, actions and rewards).</em></p></blockquote>
      <p>
      then the ith element of $n_{sa}^{h}$ can be though as the count of sucessful agents that transition to the state $s^{\prime}_{i}$ from state $s$ for  $h\leq t$. Note, we are assuming we have perfect informtion $\mathbb{F}_{t}$, hence if we have a dataset of known transitions and outcomes (gathered through self-play, expert examples, ect.) that can approximate $\mathbb{F}_{t}$ then we can define a posterior over the $p_{sa}^{h}$ that we will define as $\widehat{p_{sa}^{h}}$. An important quantity here is $w:=1/n_{sa}^{h}$ given its roll in the posterior variance given by:
      </p>
      <dir> $$\mathbf{\text{var}}_{t}(\widehat{p_{sas^{\prime}}^{h}})\leq w\mathbb{E}_{t}[\widehat{p_{sas^{\prime}}^{h}}]$$
      </dir>
      <p>where the derivation of which is of course found in the paper. To go into any further detail would require an understanding of the bellman equation and its solutions which is beyond the scope of if this post. Nonetheless, given the above upper bound of the variance, the authors postulate a posterior distribution over the robust dqn values $\bar{Q}$ as:</p>
      <div>
      $$\bar{Q}_{\text{DQN}}\sim N(\bar{Q},\mathbf{diag}(w))$$
      </div>
      <p>
which is done using the <em>Baysian Central Limit Theorem </em> which necessitates that the posterior distribution converges to a Gaussian distribution. Hence we have the theoretical intuition that: the q-values are normally distributed under certain smoothness assumptions such that its distribution is dependent on $w=1/n_{sa}^{h}$. This intuition leads us finally to the URBE algorithm.
      </p>
<img src="dqnurbepc.png" alt="pseudocode"style="width: 80%;" class="center" /><figcaption>Fig. 2. Pseudocode for the DQN-URBE as used as presented in Derman et. al. (2019) - <a href="https://arxiv.org/pdf/1905.08188.pdf" target="_blank">see the original paper here</a></figcaption>
<p>Where the update to $\sum_{a}$ is given by the Sherman-Morrison-Woodbury formula: $\sum_{a}^{next}:=\sum_{a}-(\sum_{a}\phi \phi^{T})/(1+\phi^{T}\sum_{a}\phi)$, where $\phi$ is the output vector of a hidden layer of a neural net of the active agent seperate from the q-value and the policy outuputs.</p>
      <p>DQN-URBE agents are robust explorers. By probalistically sampling actions from the distribution of Q-values, DQN-URBE agents use the network's uncertanty of the current state as a tool for action selection, which leads to exploration. Additionaly, DQN-URBE also uses Robust-DQN to regularise the Q-values which leads to robust action sampling. Unlike the case of paring epsilon-greedy and DQN, the underlying assumptions of the of URBE and robust-DQN are compatible. Namely, URBE difines an estimate for the Q-values produced by a DQN (and hence robust-DQN) network. Where epsilon-greedy is often applied without full consideration of how it changes the underlying assumptions of various frameworks, given its popularity.
</p>
<h1 id="band-exp">Bandit Exploration - A Less Greedy Approach
<a hidden class="anchor" aria-hidden="true" href="#band-exp">#</a></h1>
<p>
In the paper “When Should Agents Explore?” Pislar et. al. (2022) demonstrate that agents can perform better when a designated time period for exploration. Periods can occur multiple times per episode and can be independent of the states themselves. Moreover, mechanisms independent of the tasks or goals of the agent can trigger an exploratory period. The authors propose a couple of such triggers, albeit for only a fixed number of steps. One such trigger only activates if the discrepancy between the predicted values of past states is much higher compared to the value of the present state. Such a discrepancy suggests that the current state is unexpected by the agent in past time steps.  Hence, exploring the current state as much as possible is reasonable as the agent is in unfamiliar territory. However, the best-performing exploration trigger was a “non-stationary multi-armed bandit” approach where the triggering of an exploratory period is probabilistically determined based on the number of steps after the last exploratory period. During such, actions are picked uniformly at random throughout the entirety of the period.</p>
      <p>
Here will only focus on the aforementioned “non-stationary multi-armed bandit” approach as it is as it often yields the best results, simple to implement, and is relevant to our discussion of exploration. As this approach focuses on the ‘when’ of exploration but does not consider the behavior of the system during exploration. Thus, the approach is still considered somewhat myopic as once the agent is in exploration mode, the actions are selected uniformly at random. From now on we will refer to this approach as “mode switching” or MS.
</p>

<img src="wsa-explor.png" alt="WSA summary" class="center"/>
<figcaption>Fig. 1. A visulazation of the the MS apprach as presented in Pislar et. al. (2022) - <a href="https://arxiv.org/pdf/2108.11811.pdf" target="_blank">see the original paper here</a></figcaption>
      
<h2 id="experiments">Experiments<a hidden class="anchor" aria-hidden="true" href="#experiments">#finnaly</a></h2>
      <p>We compare the three techniques: the bayesian approach (DQN-URBE), the probabilistic bandit approach (MS), and epsilon greedy (DQN). All three generate their own training data through self-play and optimize for the probability of winning. All agents use the original DQN framework with exception of DQN-URBE (whose training algo given above) for 300000 updates to the neural network over a total of 180000 games of Backgammon. The results of which are shown below:
<img src="corr-scores.png" alt="winning corr summary">
</p>
      <p>Above is the heatmap of the winning probabilities in every 1-to-1 matchup. In the frameworks that select random actions with a probability parameter $\epsilon$ (i.e. MS and DQN) we included an agent with a different value for $\epsilon$. Similarly, for DQN-URBE we include a different agent for a different value of $\beta$, which controls the strength of the white noise applied to the q-values at infference time.</p>
      <p>Often In 1-vs-1 games like chess and tennis, a numerical rating system or "elo" is used as an objective measure of skill that can predict the best player in a match-up even if the players have not competed before. Aside from just reporting the winning probabilities, I found it more interesting to develop an elo for each of the agents.  Indeed, using an elo system gives a better sense of which agents are similar in strength and gives a better sense of which are the best and worst. 

<img src="elo.png" alt="winning corr summary">
</p>
      <p>The above elo’s were plotted over 200 games time using the elo system typically used in chess. Games were between two randomly selected agents where each agent’s elo was initialized at 800. From this we can observe very clear groups emerging. By far the best model is DQN-URBE with $\beta =0$ even though $\beta =.5$ for all DQN-URBE when producing training data during self-play. DQN-URBE agents  with any other value for $\beta$ greater than 0 play with similar elo / strength. All other agents’ elo are grouped together at the bottom.</p>
 
      <h1 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h1>
      <p>When faced with a new board position, exploration is key. Trying new moves is key to finding the most robust actions in environments that involve a high degree of randomness like in games of backgammon. Here we introduced Robust-DQN which tries to directly introduce robustness to the worst possible outcome. In addition, we introduced DQN-URBE, an extension to the Robust-DQN framework, that can take advantage of a network's uncertainty about a position and make Robust-DQN less conservative.  Finally, we introduced the MS approach to exploration, which argues for the need for "exploration periods" during training.  However,  the base framework for all these mentioned methods: DQN, does not directly look more than 1-2 moves into the future of the position. DQN instead relies on Q-values to estimate all future possible outcomes from a given action, which sounds absurd in games with dice.  Hence, future work could focus on methods that could directly observe many possible future positions like in Alpha-Zero or even MU-Zero.  (foreshadowing)</p>
      
<h1 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h1>
<p>[1] <a href="https://arxiv.org/pdf/1812.05720.pdf">Hein, M., Andriushchenko, M., & Bitterwolf, J. (2019). Why ReLU Networks Yield High-Confidence Predictions Far Away From the Training Data and How to Mitigate the Problem. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 41-50.</a></p>
<p>[2] <a href="https://arxiv.org/pdf/2108.11811.pdf">Pislar, M., Szepesvari, D., Ostrovski, G., Borsa, D., & Schaul, T. (2022). When should agents explore? ArXiv, abs/2108.11811.</a></p>
<p>[3] <a href="https://arxiv.org/pdf/1703.02310.pdf">Shashua, S.D., & Mannor, S. (2017). Deep Robust Kalman Filter. ArXiv, abs/1703.02310.</a></p>

  </div>

  <footer class="post-footer">
    <!-- TAGS
    <ul class="post-tags">
      <li><a href="https://ninalgad.github.io/tags/language-model/">language-model</a></li>
      <li><a href="https://ninalgad.github.io/tags/vision-language-model/">vision-language-model</a></li>
      <li><a href="https://ninalgad.github.io/tags/vision-model/">vision-model</a></li>
    </ul>
    -->

<!-- NEXT PAGE NAV
<nav class="paginav">
  <a class="next" href="https://lilianweng.github.io/posts/2022-04-15-data-gen/">
    <span class="title"> »</span>
    <br>
    <span>Learning with not Enough Data Part 3: Data Generation</span>
  </a>
</nav>
-->

<!-- SHARE BUTTONS
<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share Improving Datacenters Energy Efficiency on twitter"
        href="https://twitter.com/intent/tweet/?text=Generalized%20Visual%20Language%20Models&amp;url=https%3a%2f%2flilianweng.github.io%2fposts%2f2022-06-09-vlm%2f&amp;hashtags=language-model%2cvision-language-model%2cvision-model">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Improving Datacenters Energy Efficiency on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2flilianweng.github.io%2fposts%2f2022-06-09-vlm%2f&amp;title=Generalized%20Visual%20Language%20Models&amp;summary=Generalized%20Visual%20Language%20Models&amp;source=https%3a%2f%2flilianweng.github.io%2fposts%2f2022-06-09-vlm%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Improving Datacenters Energy Efficiency on reddit"
        href="https://reddit.com/submit?url=https%3a%2f%2flilianweng.github.io%2fposts%2f2022-06-09-vlm%2f&title=Generalized%20Visual%20Language%20Models">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Improving Datacenters Energy Efficiency on facebook"
        href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2flilianweng.github.io%2fposts%2f2022-06-09-vlm%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Improving Datacenters Energy Efficiency on whatsapp"
        href="https://api.whatsapp.com/send?text=Generalized%20Visual%20Language%20Models%20-%20https%3a%2f%2flilianweng.github.io%2fposts%2f2022-06-09-vlm%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Improving Datacenters Energy Efficiency on telegram"
        href="https://telegram.me/share/url?text=Generalized%20Visual%20Language%20Models&amp;url=https%3a%2f%2flilianweng.github.io%2fposts%2f2022-06-09-vlm%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>
-->
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2022 <a href="https://ninalgad.github.io/">Green Learning</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'copy';

        function copyingDone() {
            copybutton.innerText = 'copied!';
            setTimeout(() => {
                copybutton.innerText = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
